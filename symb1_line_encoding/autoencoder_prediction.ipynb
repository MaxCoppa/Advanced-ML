{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3ed981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/home/onyxia/work/Advanced-ML\")\n",
    "from data_loader import S3ParquetReader\n",
    "from config import USER\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from models import AutoEncoder, train_model, CombinedLoss\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167b6b4",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f005116",
   "metadata": {},
   "source": [
    "Config initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb5fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = f\"/{USER}/jane_street_data\"\n",
    "reader = S3ParquetReader(bucket=BUCKET)\n",
    "FILE_KEY_S3 = \"preprocessed.parquet/data_clean_symb_1.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48677b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 78)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>time_id</th><th>symbol_id</th><th>weight</th><th>feature_05</th><th>feature_06</th><th>feature_07</th><th>feature_08</th><th>feature_12</th><th>feature_13</th><th>feature_14</th><th>feature_15</th><th>feature_16</th><th>feature_17</th><th>feature_18</th><th>feature_19</th><th>feature_20</th><th>feature_22</th><th>feature_23</th><th>feature_24</th><th>feature_25</th><th>feature_28</th><th>feature_29</th><th>feature_30</th><th>feature_32</th><th>feature_33</th><th>feature_34</th><th>feature_35</th><th>feature_36</th><th>feature_37</th><th>feature_38</th><th>feature_40</th><th>feature_41</th><th>feature_43</th><th>feature_44</th><th>feature_45</th><th>feature_46</th><th>&hellip;</th><th>feature_52</th><th>feature_54</th><th>feature_55</th><th>feature_56</th><th>feature_57</th><th>feature_58</th><th>feature_59</th><th>feature_60</th><th>feature_61</th><th>feature_62</th><th>feature_63</th><th>feature_64</th><th>feature_65</th><th>feature_66</th><th>feature_67</th><th>feature_68</th><th>feature_69</th><th>feature_70</th><th>feature_71</th><th>feature_72</th><th>feature_73</th><th>feature_74</th><th>feature_75</th><th>feature_76</th><th>feature_77</th><th>feature_78</th><th>responder_0</th><th>responder_1</th><th>responder_2</th><th>responder_3</th><th>responder_4</th><th>responder_5</th><th>responder_6</th><th>responder_7</th><th>responder_8</th><th>feature_time_sin</th><th>feature_time_cos</th></tr><tr><td>i16</td><td>i16</td><td>i8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>35</td><td>1</td><td>3.889038</td><td>0.127298</td><td>0.339121</td><td>-0.045494</td><td>0.151473</td><td>-0.038995</td><td>-0.644192</td><td>-0.193971</td><td>-0.016662</td><td>-0.213108</td><td>-0.045016</td><td>0.014899</td><td>0.128033</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.397016</td><td>0.694868</td><td>0.812098</td><td>-0.128621</td><td>0.025559</td><td>0.018694</td><td>-0.006306</td><td>-1.706433</td><td>-0.490941</td><td>0.425017</td><td>0.281025</td><td>-0.134582</td><td>0.046605</td><td>&hellip;</td><td>-0.207253</td><td>-0.307967</td><td>0.381731</td><td>0.756464</td><td>0.058235</td><td>0.060041</td><td>-0.479444</td><td>-0.000608</td><td>0.0</td><td>-0.000122</td><td>0.085259</td><td>-0.324355</td><td>-0.097414</td><td>-0.296044</td><td>0.21344</td><td>-0.20025</td><td>-0.11987</td><td>-0.007101</td><td>-0.693186</td><td>-0.181427</td><td>0.073886</td><td>-0.062266</td><td>0.136151</td><td>0.110356</td><td>0.055748</td><td>-0.028266</td><td>0.36224</td><td>-0.376568</td><td>-0.086291</td><td>-0.392552</td><td>-0.267725</td><td>-0.112329</td><td>-1.233082</td><td>0.131704</td><td>-0.024239</td><td>0.225461</td><td>0.974252</td></tr><tr><td>0</td><td>36</td><td>1</td><td>3.889038</td><td>0.114208</td><td>-1.419921</td><td>-0.505525</td><td>-0.258195</td><td>0.017889</td><td>-0.254385</td><td>0.015691</td><td>-0.020583</td><td>-0.15111</td><td>-0.034961</td><td>-0.045215</td><td>-0.411787</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.037745</td><td>-0.824408</td><td>-0.312367</td><td>0.272155</td><td>0.041673</td><td>0.051002</td><td>0.101001</td><td>-0.172871</td><td>0.356697</td><td>0.43781</td><td>-0.243426</td><td>0.116691</td><td>0.21925</td><td>&hellip;</td><td>0.174336</td><td>-0.325709</td><td>-0.301785</td><td>-0.332396</td><td>0.651199</td><td>-0.103911</td><td>-0.210683</td><td>0.088785</td><td>0.0</td><td>-0.013245</td><td>-0.067586</td><td>0.22172</td><td>0.347892</td><td>0.511532</td><td>-0.214167</td><td>-0.045423</td><td>-0.048534</td><td>0.048807</td><td>-0.407794</td><td>0.008081</td><td>-0.071584</td><td>0.056597</td><td>0.01653</td><td>-0.049644</td><td>-0.023103</td><td>-0.0531</td><td>0.283566</td><td>-0.397785</td><td>0.041572</td><td>0.001317</td><td>-0.112683</td><td>0.419467</td><td>-0.578848</td><td>0.269364</td><td>0.459819</td><td>0.231787</td><td>0.972767</td></tr><tr><td>0</td><td>37</td><td>1</td><td>3.889038</td><td>0.118826</td><td>-1.293755</td><td>-0.809856</td><td>0.268584</td><td>-0.056675</td><td>-0.273341</td><td>-0.016404</td><td>0.010669</td><td>0.172457</td><td>0.011381</td><td>0.013246</td><td>-0.016248</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.571682</td><td>0.168574</td><td>-0.042331</td><td>-0.55658</td><td>0.175519</td><td>-0.052067</td><td>0.053939</td><td>0.109843</td><td>-0.055707</td><td>-0.214608</td><td>-0.026042</td><td>-0.180718</td><td>-0.171372</td><td>&hellip;</td><td>-1.097941</td><td>-0.294437</td><td>-0.104787</td><td>0.504722</td><td>-0.285762</td><td>0.140226</td><td>-0.521822</td><td>-0.46183</td><td>0.0</td><td>0.226184</td><td>-0.100162</td><td>-0.029472</td><td>0.037072</td><td>0.097566</td><td>0.061552</td><td>-0.136708</td><td>-0.029742</td><td>-0.047774</td><td>-0.150518</td><td>-0.287006</td><td>-0.000054</td><td>-0.048201</td><td>-0.093462</td><td>-0.015866</td><td>0.051347</td><td>0.055587</td><td>0.260547</td><td>-0.299865</td><td>0.239627</td><td>-0.019651</td><td>-0.194262</td><td>0.321798</td><td>-0.683034</td><td>0.027111</td><td>0.075722</td><td>0.238102</td><td>0.97124</td></tr><tr><td>0</td><td>38</td><td>1</td><td>3.889038</td><td>-0.216346</td><td>-0.021069</td><td>-0.425764</td><td>-0.005701</td><td>-0.010654</td><td>0.302838</td><td>0.000491</td><td>-0.017648</td><td>-0.036415</td><td>-0.008958</td><td>0.384812</td><td>0.49453</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.202302</td><td>0.395804</td><td>-0.512155</td><td>0.642004</td><td>0.414221</td><td>0.113961</td><td>-0.131998</td><td>-0.374575</td><td>0.206433</td><td>-0.445924</td><td>-0.518108</td><td>0.268771</td><td>0.069906</td><td>&hellip;</td><td>0.08609</td><td>-0.610097</td><td>0.137679</td><td>-0.198983</td><td>-0.754921</td><td>-0.294851</td><td>0.352242</td><td>0.049567</td><td>0.0</td><td>-0.023089</td><td>0.036929</td><td>-0.176699</td><td>-0.503255</td><td>-0.449954</td><td>-0.134894</td><td>0.050516</td><td>-0.089379</td><td>0.004014</td><td>-0.204778</td><td>-0.144482</td><td>-0.082102</td><td>-0.050271</td><td>0.020269</td><td>-0.008658</td><td>0.096835</td><td>-0.042524</td><td>0.423799</td><td>-0.195033</td><td>0.145029</td><td>-0.076218</td><td>-0.304814</td><td>0.098249</td><td>-0.606483</td><td>-0.01076</td><td>-0.134628</td><td>0.244408</td><td>0.969673</td></tr><tr><td>0</td><td>39</td><td>1</td><td>3.889038</td><td>-0.660073</td><td>2.168059</td><td>0.903407</td><td>-0.101644</td><td>0.133174</td><td>-0.212203</td><td>-0.217244</td><td>0.006182</td><td>-0.076704</td><td>-0.027879</td><td>0.249095</td><td>-0.431726</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.511043</td><td>-0.221639</td><td>-0.014473</td><td>-0.572408</td><td>-0.297749</td><td>-0.126506</td><td>-0.083542</td><td>0.180433</td><td>-0.245343</td><td>0.052368</td><td>0.051568</td><td>-0.067317</td><td>0.585641</td><td>&hellip;</td><td>0.422847</td><td>1.033209</td><td>-0.497286</td><td>-0.080836</td><td>0.111428</td><td>0.351126</td><td>0.692625</td><td>0.136429</td><td>0.0</td><td>-0.158061</td><td>0.19554</td><td>0.039516</td><td>0.159207</td><td>0.312143</td><td>0.071603</td><td>0.039128</td><td>-0.042775</td><td>-0.063419</td><td>0.105373</td><td>-0.419445</td><td>0.211242</td><td>0.070223</td><td>-0.057609</td><td>-0.028877</td><td>-0.147115</td><td>0.014544</td><td>0.245084</td><td>-0.368908</td><td>0.058499</td><td>-0.083503</td><td>-0.257635</td><td>-0.007915</td><td>-0.446952</td><td>0.085893</td><td>-0.124889</td><td>0.250703</td><td>0.968064</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 78)\n",
       "┌─────────┬─────────┬───────────┬──────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ date_id ┆ time_id ┆ symbol_id ┆ weight   ┆ … ┆ responder_ ┆ responder_ ┆ feature_ti ┆ feature_ti │\n",
       "│ ---     ┆ ---     ┆ ---       ┆ ---      ┆   ┆ 7          ┆ 8          ┆ me_sin     ┆ me_cos     │\n",
       "│ i16     ┆ i16     ┆ i8        ┆ f32      ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
       "│         ┆         ┆           ┆          ┆   ┆ f32        ┆ f32        ┆ f64        ┆ f64        │\n",
       "╞═════════╪═════════╪═══════════╪══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ 0       ┆ 35      ┆ 1         ┆ 3.889038 ┆ … ┆ 0.131704   ┆ -0.024239  ┆ 0.225461   ┆ 0.974252   │\n",
       "│ 0       ┆ 36      ┆ 1         ┆ 3.889038 ┆ … ┆ 0.269364   ┆ 0.459819   ┆ 0.231787   ┆ 0.972767   │\n",
       "│ 0       ┆ 37      ┆ 1         ┆ 3.889038 ┆ … ┆ 0.027111   ┆ 0.075722   ┆ 0.238102   ┆ 0.97124    │\n",
       "│ 0       ┆ 38      ┆ 1         ┆ 3.889038 ┆ … ┆ -0.01076   ┆ -0.134628  ┆ 0.244408   ┆ 0.969673   │\n",
       "│ 0       ┆ 39      ┆ 1         ┆ 3.889038 ┆ … ┆ 0.085893   ┆ -0.124889  ┆ 0.250703   ┆ 0.968064   │\n",
       "└─────────┴─────────┴───────────┴──────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = reader.read_parquet(FILE_KEY_S3)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa28aacd",
   "metadata": {},
   "source": [
    "Define features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77997c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"responder_6\"\n",
    "features = [col for col in data.columns if \"feature\" in col]\n",
    "\n",
    "X, y = data[features], data[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594e16d",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "742c1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.height\n",
    "n_train = int(0.8 * n)\n",
    "X_train = X.slice(0, n_train)\n",
    "y_train = y.slice(0, n_train)\n",
    "\n",
    "X_val = X.slice(n_train)\n",
    "y_val = y.slice(n_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06672e60",
   "metadata": {},
   "source": [
    "Data Wraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0cbacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
    "        y = torch.tensor(y.to_numpy(), dtype=torch.float32)\n",
    "        if y.ndim == 1:\n",
    "            y = y.view(-1, 1)\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47867e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "val_dataset = TimeSeriesDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b993691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(\n",
    "    train_dataset, batch_size=2048, shuffle=False, num_workers=8\n",
    ")\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=2048, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8680c22",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20d494e",
   "metadata": {},
   "source": [
    "Model hyperameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3822321",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat = len(features)\n",
    "n_latent = 16\n",
    "encoder_hidden = [64, 32, 32]\n",
    "decoder_hidden = [32, 64]\n",
    "head_hidden = [8, 8, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eea4ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(\n",
    "    n_feat=n_feat,\n",
    "    n_latent=n_latent,\n",
    "    encoder_hidden=encoder_hidden,\n",
    "    decoder_hidden=decoder_hidden,\n",
    "    head_hidden=head_hidden,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d260b0",
   "metadata": {},
   "source": [
    "Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9267c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "alpha = 1.0\n",
    "beta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e25dc99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "criterion = CombinedLoss(alpha=alpha, beta=beta)\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e179a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/Advanced-ML/models/train_model.py:44\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, device, n_epochs, title)\u001b[39m\n\u001b[32m     40\u001b[39m optimizer.zero_grad()\n\u001b[32m     42\u001b[39m _, x_hat, y_hat = model(xb)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m loss.backward()\n\u001b[32m     46\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/Advanced-ML/dl_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/Advanced-ML/dl_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/Advanced-ML/models/losses.py:13\u001b[39m, in \u001b[36mCombinedLoss.forward\u001b[39m\u001b[34m(self, x_hat, x_true, y_hat, y_true)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_hat, x_true, y_hat, y_true):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     loss_rec = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     loss_sup = F.mse_loss(y_hat, y_true)\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.alpha * loss_rec + \u001b[38;5;28mself\u001b[39m.beta * loss_sup\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/Advanced-ML/dl_env/lib/python3.13/site-packages/torch/nn/functional.py:3885\u001b[39m, in \u001b[36mmse_loss\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction, weight)\u001b[39m\n\u001b[32m   3881\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3882\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid reduction mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Expected one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33msum\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3883\u001b[39m         )\n\u001b[32m   3884\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3885\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3886\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3887\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_data_loader,\n",
    "    val_loader=val_data_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    n_epochs=n_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2e1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
