{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c2615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data_loader import S3ParquetReader\n",
    "from config import USER\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from models import AutoEncoder, train_model, CombinedLoss\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167b6b4",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f005116",
   "metadata": {},
   "source": [
    "Config initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb5fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = f\"/{USER}/jane_street_data\"\n",
    "reader = S3ParquetReader(bucket=BUCKET)\n",
    "FILE_KEY_S3 = \"preprocessed.parquet/data_clean_symb_1.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48677b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 79)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>time_id</th><th>symbol_id</th><th>weight</th><th>feature_05</th><th>feature_06</th><th>feature_07</th><th>feature_08</th><th>feature_09</th><th>feature_10</th><th>feature_11</th><th>feature_12</th><th>feature_13</th><th>feature_14</th><th>feature_15</th><th>feature_16</th><th>feature_17</th><th>feature_18</th><th>feature_19</th><th>feature_20</th><th>feature_22</th><th>feature_23</th><th>feature_24</th><th>feature_25</th><th>feature_28</th><th>feature_29</th><th>feature_30</th><th>feature_32</th><th>feature_33</th><th>feature_34</th><th>feature_35</th><th>feature_36</th><th>feature_37</th><th>feature_38</th><th>feature_40</th><th>feature_41</th><th>feature_43</th><th>&hellip;</th><th>feature_49</th><th>feature_51</th><th>feature_52</th><th>feature_54</th><th>feature_55</th><th>feature_56</th><th>feature_57</th><th>feature_58</th><th>feature_59</th><th>feature_60</th><th>feature_61</th><th>feature_62</th><th>feature_63</th><th>feature_64</th><th>feature_65</th><th>feature_66</th><th>feature_67</th><th>feature_68</th><th>feature_69</th><th>feature_70</th><th>feature_71</th><th>feature_72</th><th>feature_73</th><th>feature_74</th><th>feature_75</th><th>feature_76</th><th>feature_77</th><th>feature_78</th><th>responder_0</th><th>responder_1</th><th>responder_2</th><th>responder_3</th><th>responder_4</th><th>responder_5</th><th>responder_6</th><th>responder_7</th><th>responder_8</th></tr><tr><td>i16</td><td>i16</td><td>i8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i8</td><td>i8</td><td>i16</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>34</td><td>1</td><td>3.889038</td><td>1.509601</td><td>0.885045</td><td>2.831856</td><td>-1.201967</td><td>11</td><td>7</td><td>76</td><td>-0.271028</td><td>0.695742</td><td>0.794157</td><td>-0.038239</td><td>0.045858</td><td>0.030772</td><td>-0.900359</td><td>-0.382682</td><td>0.91013</td><td>1.636431</td><td>1.522133</td><td>-1.551398</td><td>-0.229627</td><td>1.378301</td><td>-0.283712</td><td>0.123196</td><td>0.586801</td><td>0.250828</td><td>0.294883</td><td>0.521403</td><td>-0.682683</td><td>-0.239552</td><td>-0.460341</td><td>1.955693</td><td>1.485422</td><td>-0.253407</td><td>&hellip;</td><td>1.380354</td><td>1.116463</td><td>1.92235</td><td>0.375684</td><td>-1.258121</td><td>-0.705236</td><td>0.748627</td><td>0.643394</td><td>0.627821</td><td>1.411739</td><td>-1.36224</td><td>0.414762</td><td>0.683652</td><td>0.896076</td><td>-1.06081</td><td>-0.636584</td><td>-0.423285</td><td>0.055839</td><td>0.424335</td><td>-0.108809</td><td>1.035421</td><td>1.629745</td><td>-0.407497</td><td>-0.345388</td><td>-0.342097</td><td>-0.281208</td><td>-0.406524</td><td>-0.258637</td><td>0.573992</td><td>-0.319932</td><td>-0.335376</td><td>-0.21128</td><td>-0.279558</td><td>-0.434355</td><td>-1.220439</td><td>0.214211</td><td>-0.257941</td></tr><tr><td>0</td><td>35</td><td>1</td><td>3.889038</td><td>1.636899</td><td>1.224167</td><td>2.786362</td><td>-1.050494</td><td>11</td><td>7</td><td>76</td><td>-0.310024</td><td>0.05155</td><td>0.600186</td><td>-0.054901</td><td>-0.167249</td><td>-0.014244</td><td>-0.885459</td><td>-0.254649</td><td>0.91013</td><td>1.636431</td><td>1.522133</td><td>-1.551398</td><td>-0.229627</td><td>1.378301</td><td>-0.283712</td><td>0.123196</td><td>0.189785</td><td>0.945696</td><td>1.106982</td><td>0.392782</td><td>-0.657124</td><td>-0.220858</td><td>-0.466647</td><td>0.24926</td><td>0.994481</td><td>0.17161</td><td>&hellip;</td><td>1.066112</td><td>-1.037891</td><td>1.715097</td><td>0.067716</td><td>-0.876391</td><td>0.051228</td><td>0.806862</td><td>0.703435</td><td>0.148378</td><td>1.411131</td><td>-1.36224</td><td>0.41464</td><td>0.768911</td><td>0.571721</td><td>-1.158224</td><td>-0.932629</td><td>-0.209845</td><td>-0.144411</td><td>0.304465</td><td>-0.11591</td><td>0.342235</td><td>1.448319</td><td>-0.333611</td><td>-0.407654</td><td>-0.205946</td><td>-0.170852</td><td>-0.350776</td><td>-0.286903</td><td>0.36224</td><td>-0.376568</td><td>-0.086291</td><td>-0.392552</td><td>-0.267725</td><td>-0.112329</td><td>-1.233082</td><td>0.131704</td><td>-0.024239</td></tr><tr><td>0</td><td>36</td><td>1</td><td>3.889038</td><td>1.751107</td><td>-0.195755</td><td>2.280836</td><td>-1.308689</td><td>11</td><td>7</td><td>76</td><td>-0.292135</td><td>-0.202836</td><td>0.615877</td><td>-0.075484</td><td>-0.318359</td><td>-0.049205</td><td>-0.930674</td><td>-0.666436</td><td>0.91013</td><td>1.636431</td><td>1.522133</td><td>-1.551398</td><td>-0.229627</td><td>1.378301</td><td>-0.283712</td><td>0.123196</td><td>0.15204</td><td>0.121289</td><td>0.794614</td><td>0.664937</td><td>-0.615451</td><td>-0.169856</td><td>-0.365646</td><td>0.076389</td><td>1.351177</td><td>0.60942</td><td>&hellip;</td><td>1.095427</td><td>-1.714377</td><td>1.889433</td><td>-0.257992</td><td>-1.178176</td><td>-0.281169</td><td>1.45806</td><td>0.599523</td><td>-0.062305</td><td>1.499916</td><td>-1.36224</td><td>0.401396</td><td>0.701324</td><td>0.793441</td><td>-0.810332</td><td>-0.421096</td><td>-0.424012</td><td>-0.189834</td><td>0.255931</td><td>-0.067103</td><td>-0.06556</td><td>1.456399</td><td>-0.405195</td><td>-0.351057</td><td>-0.189416</td><td>-0.220496</td><td>-0.373879</td><td>-0.340003</td><td>0.283566</td><td>-0.397785</td><td>0.041572</td><td>0.001317</td><td>-0.112683</td><td>0.419467</td><td>-0.578848</td><td>0.269364</td><td>0.459819</td></tr><tr><td>0</td><td>37</td><td>1</td><td>3.889038</td><td>1.869932</td><td>-1.48951</td><td>1.470981</td><td>-1.040105</td><td>11</td><td>7</td><td>76</td><td>-0.34881</td><td>-0.476177</td><td>0.599473</td><td>-0.064814</td><td>-0.145902</td><td>-0.037824</td><td>-0.917428</td><td>-0.682684</td><td>0.91013</td><td>1.636431</td><td>1.522133</td><td>-1.551398</td><td>-0.229627</td><td>1.378301</td><td>-0.283712</td><td>0.123196</td><td>0.723722</td><td>0.289863</td><td>0.752283</td><td>0.108357</td><td>-0.439932</td><td>-0.221922</td><td>-0.311707</td><td>0.186232</td><td>1.29547</td><td>0.394812</td><td>&hellip;</td><td>0.979125</td><td>1.078464</td><td>0.791492</td><td>-0.55243</td><td>-1.282962</td><td>0.223554</td><td>1.172299</td><td>0.73975</td><td>-0.584127</td><td>1.038086</td><td>-1.36224</td><td>0.627579</td><td>0.601162</td><td>0.763969</td><td>-0.77326</td><td>-0.32353</td><td>-0.36246</td><td>-0.326542</td><td>0.226188</td><td>-0.114877</td><td>-0.216078</td><td>1.169394</td><td>-0.405249</td><td>-0.399258</td><td>-0.282878</td><td>-0.236362</td><td>-0.322532</td><td>-0.284416</td><td>0.260547</td><td>-0.299865</td><td>0.239627</td><td>-0.019651</td><td>-0.194262</td><td>0.321798</td><td>-0.683034</td><td>0.027111</td><td>0.075722</td></tr><tr><td>0</td><td>38</td><td>1</td><td>3.889038</td><td>1.653586</td><td>-1.510579</td><td>1.045217</td><td>-1.045805</td><td>11</td><td>7</td><td>76</td><td>-0.359463</td><td>-0.173339</td><td>0.599964</td><td>-0.082462</td><td>-0.182317</td><td>-0.046782</td><td>-0.532617</td><td>-0.188155</td><td>0.91013</td><td>1.636431</td><td>1.522133</td><td>-1.551398</td><td>-0.229627</td><td>1.378301</td><td>-0.283712</td><td>0.123196</td><td>0.52142</td><td>0.685667</td><td>0.240129</td><td>0.750362</td><td>-0.025711</td><td>-0.107961</td><td>-0.443705</td><td>-0.188343</td><td>1.501904</td><td>-0.051112</td><td>&hellip;</td><td>0.712261</td><td>0.803482</td><td>0.877582</td><td>-1.162526</td><td>-1.145284</td><td>0.024571</td><td>0.417377</td><td>0.444899</td><td>-0.231885</td><td>1.087653</td><td>-1.36224</td><td>0.60449</td><td>0.638092</td><td>0.58727</td><td>-1.276514</td><td>-0.773484</td><td>-0.497354</td><td>-0.276027</td><td>0.13681</td><td>-0.110864</td><td>-0.420856</td><td>1.024912</td><td>-0.48735</td><td>-0.449529</td><td>-0.262609</td><td>-0.24502</td><td>-0.225697</td><td>-0.326941</td><td>0.423799</td><td>-0.195033</td><td>0.145029</td><td>-0.076218</td><td>-0.304814</td><td>0.098249</td><td>-0.606483</td><td>-0.01076</td><td>-0.134628</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 79)\n",
       "┌─────────┬─────────┬───────────┬──────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ date_id ┆ time_id ┆ symbol_id ┆ weight   ┆ … ┆ responder_ ┆ responder_ ┆ responder_ ┆ responder_ │\n",
       "│ ---     ┆ ---     ┆ ---       ┆ ---      ┆   ┆ 5          ┆ 6          ┆ 7          ┆ 8          │\n",
       "│ i16     ┆ i16     ┆ i8        ┆ f32      ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
       "│         ┆         ┆           ┆          ┆   ┆ f32        ┆ f32        ┆ f32        ┆ f32        │\n",
       "╞═════════╪═════════╪═══════════╪══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ 0       ┆ 34      ┆ 1         ┆ 3.889038 ┆ … ┆ -0.434355  ┆ -1.220439  ┆ 0.214211   ┆ -0.257941  │\n",
       "│ 0       ┆ 35      ┆ 1         ┆ 3.889038 ┆ … ┆ -0.112329  ┆ -1.233082  ┆ 0.131704   ┆ -0.024239  │\n",
       "│ 0       ┆ 36      ┆ 1         ┆ 3.889038 ┆ … ┆ 0.419467   ┆ -0.578848  ┆ 0.269364   ┆ 0.459819   │\n",
       "│ 0       ┆ 37      ┆ 1         ┆ 3.889038 ┆ … ┆ 0.321798   ┆ -0.683034  ┆ 0.027111   ┆ 0.075722   │\n",
       "│ 0       ┆ 38      ┆ 1         ┆ 3.889038 ┆ … ┆ 0.098249   ┆ -0.606483  ┆ -0.01076   ┆ -0.134628  │\n",
       "└─────────┴─────────┴───────────┴──────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = reader.read_parquet(FILE_KEY_S3)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf88b7",
   "metadata": {},
   "source": [
    "Ensure sorted by time_id to create batches not leaked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae530ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort(by=[\"date_id\",\"time_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa28aacd",
   "metadata": {},
   "source": [
    "Define features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77997c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"responder_6\"\n",
    "features = [col for col in data.columns if \"feature\" in col]\n",
    "\n",
    "X, y = data[features], data[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594e16d",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "742c1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.height\n",
    "n_train = int(0.8 * n)\n",
    "X_train = X.slice(0, n_train)\n",
    "y_train = y.slice(0, n_train)\n",
    "\n",
    "X_val = X.slice(n_train)\n",
    "y_val = y.slice(n_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06672e60",
   "metadata": {},
   "source": [
    "Data Wraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0cbacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
    "        y = torch.tensor(y.to_numpy(), dtype=torch.float32)\n",
    "        if y.ndim == 1:\n",
    "            y = y.view(-1, 1)\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47867e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "val_dataset = TimeSeriesDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b993691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(\n",
    "    train_dataset, batch_size=2048, shuffle=False, num_workers=8\n",
    ")\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=2048, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8680c22",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20d494e",
   "metadata": {},
   "source": [
    "Model hyperameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3822321",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat = len(features)\n",
    "n_latent = 16\n",
    "encoder_hidden = [64, 32, 32]\n",
    "decoder_hidden = [32, 64]\n",
    "head_hidden = [8, 8, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eea4ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(\n",
    "    n_feat=n_feat,\n",
    "    n_latent=n_latent,\n",
    "    encoder_hidden=encoder_hidden,\n",
    "    decoder_hidden=decoder_hidden,\n",
    "    head_hidden=head_hidden,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d260b0",
   "metadata": {},
   "source": [
    "Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9267c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "device = torch.device(\"cpu\")\n",
    "alpha = 1.0\n",
    "beta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e25dc99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "criterion = CombinedLoss(alpha=alpha, beta=beta)\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e179a73",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/Advanced-ML/models/train_model.py:44\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, device, n_epochs)\u001b[39m\n\u001b[32m     41\u001b[39m _, x_hat, y_hat = model(xb)\n\u001b[32m     43\u001b[39m loss = criterion(x_hat, xb, y_hat, yb)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m optimizer.step()\n\u001b[32m     47\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/Advanced-ML/dl_env/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/Advanced-ML/dl_env/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/Advanced-ML/dl_env/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_data_loader,\n",
    "    val_loader=val_data_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    n_epochs=n_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2e1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
